{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "[0, 1, 0]\n",
      "[0, 1, 2]\n",
      "[0, 1.0, 0.0]\n",
      "[1, 2, 1]\n",
      "[13, 14, 15]\n",
      "[0.07692307692307693, 0.14285714285714285, 0.06666666666666667]\n",
      "[0, 1.0, 0.0]\n",
      "[0.07692307692307693, 0.14285714285714285, 0.06666666666666667]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stops=set(stopwords.words('english'))\n",
    "corpus=u\"<s> hello how are you doing ? Hope you find the book interesting. </s>\".split()\n",
    "sentence=u\"<s>how are you doing</s>\".split()\n",
    "stopwords = set(stopwords.words('english'))\n",
    "vocabulary=set(corpus)\n",
    "print(len(vocabulary))\n",
    "cfd = nltk.ConditionalFreqDist(nltk.bigrams(corpus))\n",
    "print([cfd[a][b] for (a,b) in nltk.bigrams(sentence)])\n",
    "print([cfd[a].N() for (a,b) in nltk.bigrams(sentence)])\n",
    "print([cfd[a].freq(b) for (a,b) in nltk.bigrams(sentence)])\n",
    "print([1 + cfd[a][b] for (a,b) in nltk.bigrams(sentence)])\n",
    "print([len(vocabulary) + cfd[a].N() for (a,b) in nltk.bigrams(sentence)])\n",
    "print([1.0 * (1+cfd[a][b]) / (len(vocabulary)+cfd[a].N()) for (a,b) in nltk.bigrams(sentence)])\n",
    "cpd_mle = nltk.ConditionalProbDist(cfd, nltk.MLEProbDist, bins=len(vocabulary))\n",
    "print([cpd_mle[a].prob(b) for (a,b) in nltk.bigrams(sentence)])\n",
    "cpd_laplace = nltk.ConditionalProbDist(cfd, nltk.LaplaceProbDist, bins=len(vocabulary))\n",
    "print([cpd_laplace[a].prob(b) for (a,b) in nltk.bigrams(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n",
      "happi\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmerporter = PorterStemmer()\n",
    "print(stemmerporter.stem('working'))\n",
    "print(stemmerporter.stem('happiness'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n",
      "happy\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmerlan=LancasterStemmer()\n",
    "print(stemmerlan.stem('working'))\n",
    "print(stemmerlan.stem('happiness'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n",
      "happiness\n",
      "pair\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import RegexpStemmer\n",
    "stemmerregexp=RegexpStemmer('ing')\n",
    "print(stemmerregexp.stem('working'))\n",
    "print(stemmerregexp.stem('happiness'))\n",
    "print(stemmerregexp.stem('pairing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n",
      "com\n",
      "mang\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "print(SnowballStemmer.languages)\n",
    "spanishstemmer=SnowballStemmer('spanish')\n",
    "print(spanishstemmer.stem('comiendo'))\n",
    "frenchstemmer=SnowballStemmer('french')\n",
    "print(frenchstemmer.stem('manger'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n",
      "work\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer_output=WordNetLemmatizer()\n",
    "print(lemmatizer_output.lemmatize('working'))\n",
    "print(lemmatizer_output.lemmatize('working',pos='v'))\n",
    "print(lemmatizer_output.lemmatize('works'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happi\n",
      "happiness\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer_output=PorterStemmer()\n",
    "print(stemmer_output.stem('happiness'))\n",
    "lemmatizer_output=WordNetLemmatizer()\n",
    "print(lemmatizer_output.lemmatize('happiness'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('It', 'PRP'), ('is', 'VBZ'), ('a', 'DT'), ('pleasant', 'JJ'), ('day', 'NN'), ('today', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text1=nltk.word_tokenize(\"It is a pleasant day today\")\n",
    "print(nltk.pos_tag(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.help.upenn_tagset('NNS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.help.upenn_tagset('VB.*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('can', 'MD'), ('not', 'RB'), ('bear', 'VB'), ('the', 'DT'), ('pain', 'NN'), ('of', 'IN'), ('bear', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text=nltk.word_tokenize(\"I cannot bear the pain of bear\")\n",
    "print(nltk.pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bear', 'NN')\n",
      "bear\n",
      "NN\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "taggedword=nltk.tag.str2tuple('bear/NN')\n",
    "print(taggedword)\n",
    "print(taggedword[0])\n",
    "print(taggedword[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('sacred', 'VBN'), ('Ganga', 'NNP'), ('flows', 'VBZ'), ('in', 'IN'), ('this', 'DT'), ('region', 'NN'), ('.', '.'), ('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('pilgrimage', 'NN'), ('.', '.'), ('People', 'NNP'), ('from', 'IN'), ('all', 'DT'), ('over', 'IN'), ('the', 'DT'), ('country', 'NN'), ('visit', 'NN'), ('this', 'DT'), ('place', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sentence='''The/DT sacred/VBN Ganga/NNP flows/VBZ in/IN this/DT region/NN ./. This/DT is/VBZ a/DT pilgrimage/NN ./. People/NNP from/IN all/DT over/IN the/DT country/NN visit/NN this/DT place/NN ./. '''\n",
    "print([nltk.tag.str2tuple(t) for t in sentence.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bear/NN\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "taggedtok = ('bear', 'NN')\n",
    "from nltk.tag.util import tuple2str\n",
    "print(tuple2str(taggedtok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NOUN', 28867), ('VERB', 13564), ('.', 11715), ('ADP', 9857), ('DET', 8725), ('X', 6613), ('ADJ', 6397), ('NUM', 3546), ('PRT', 3219), ('ADV', 3171), ('PRON', 2737), ('CONJ', 2265)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "treebank_tagged = treebank.tagged_words(tagset='universal')\n",
    "tag = nltk.FreqDist(tag for (word, tag) in treebank_tagged)\n",
    "print(tag.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOUN', 'DET', 'ADJ', 'ADP', '.', 'VERB', 'NUM', 'PRT', 'CONJ', 'PRON', 'X', 'ADV']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\t\n",
    "treebank_tagged = treebank.tagged_words(tagset='universal')\n",
    "tagpairs = nltk.bigrams(treebank_tagged)\n",
    "preceders_noun = [x[1] for (x, y) in tagpairs if y[1] == 'NOUN']\n",
    "freqdist = nltk.FreqDist(preceders_noun)\n",
    "print([tag for (tag, _) in freqdist.most_common()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'beautiful': 'ADJ', 'boy': 'N', 'read': 'V', 'generously': 'ADV'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "tag={}\n",
    "print(tag)\n",
    "tag['beautiful']='ADJ'\n",
    "\n",
    "tag['boy']='N'\n",
    "tag['read']='V'\n",
    "tag['generously']='ADV'\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Beautiful', 'NN'), ('morning', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import DefaultTagger\n",
    "tag = DefaultTagger('NN')\n",
    "print(tag.tag(['Beautiful', 'morning']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beautiful', 'morning']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import untag\n",
    "print(untag([('beautiful', 'NN'), ('morning', 'NN')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os,os.path\n",
    "create = os.path.expanduser('~/nltkdoc')\n",
    "if not os.path.exists(create):\n",
    "    os.mkdir(create)\n",
    "print(os.path.exists(create))\n",
    "import nltk.data\n",
    "print(create in nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2943\n",
      "5001\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import names\n",
    "print(len(names.words('male.txt')))\n",
    "print(len(names.words('female.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en', 'en-basic']\n",
      "235886\n",
      "850\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "print(words.fileids())\n",
    "print(len(words.words('en')))\n",
    "print(len(words.words('en-basic')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import treebank\n",
    "training= treebank.tagged_sents()[:7000]\n",
    "unitagger=UnigramTagger(training)\n",
    "print(treebank.sents()[0])\n",
    "print(unitagger.tag(treebank.sents()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9619024159944167\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tag import UnigramTagger\n",
    "training= treebank.tagged_sents()[:7000]\n",
    "unitagger=UnigramTagger(training)\n",
    "testing = treebank.tagged_sents()[2000:]\n",
    "print(unitagger.evaluate(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', None), ('Vinken', 'NN'), (',', None), ('61', None), ('years', None), ('old', None), (',', None), ('will', None), ('join', None), ('the', None), ('board', None), ('as', None), ('a', None), ('nonexecutive', None), ('director', None), ('Nov.', None), ('29', None), ('.', None)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tag import UnigramTagger\n",
    "unitag = UnigramTagger(model={'Vinken': 'NN'})\n",
    "print(unitag.tag(treebank.sents()[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9619024159944167\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.corpus import treebank\n",
    "testing = treebank.tagged_sents()[2000:]\n",
    "training= treebank.tagged_sents()[:7000]\n",
    "tag1=DefaultTagger('NN')\n",
    "tag2=UnigramTagger(training,backoff=tag1)\n",
    "print(tag2.evaluate(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "0.9171131227292321\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.corpus import treebank\n",
    "training_1= treebank.tagged_sents()[:7000]\n",
    "bigramtagger=BigramTagger(training_1)\n",
    "print(treebank.sents()[0])\n",
    "print(bigramtagger.tag(treebank.sents()[0]))\n",
    "testing_1 = treebank.tagged_sents()[2000:]\n",
    "print(bigramtagger.evaluate(testing_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9171131227292321\n",
      "0.9022107272615308\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import BigramTagger, TrigramTagger\n",
    "from nltk.corpus import treebank\n",
    "testing = treebank.tagged_sents()[2000:]\n",
    "training= treebank.tagged_sents()[:7000]\n",
    "bigramtag = BigramTagger(training)\n",
    "print(bigramtag.evaluate(testing))\n",
    "trigramtag = TrigramTagger(training)\n",
    "print(trigramtag.evaluate(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9304554878173943\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from nltk import NgramTagger\n",
    "testing = treebank.tagged_sents()[2000:]\n",
    "training= treebank.tagged_sents()[:7000]\n",
    "quadgramtag = NgramTagger(4, training)\n",
    "print(quadgramtag.evaluate(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2902682841718497\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import AffixTagger\n",
    "from nltk.corpus import treebank\n",
    "testing = treebank.tagged_sents()[2000:]\n",
    "training= treebank.tagged_sents()[:7000]\n",
    "affixtag = AffixTagger(training)\n",
    "print(affixtag.evaluate(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2094751318841472\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import AffixTagger\n",
    "from nltk.corpus import treebank\n",
    "testing = treebank.tagged_sents()[2000:]\n",
    "training= treebank.tagged_sents()[:7000]\n",
    "prefixtag = AffixTagger(training, affix_length=4)\n",
    "print(prefixtag.evaluate(testing))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2902682841718497\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import AffixTagger\n",
    "from nltk.corpus import treebank\n",
    "testing = treebank.tagged_sents()[2000:]\n",
    "training= treebank.tagged_sents()[:7000]\n",
    "suffixtag = AffixTagger(training, affix_length=-3)\n",
    "print(suffixtag.evaluate(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25841082168442225\n",
      "0.2940451998275756\n",
      "0.33072644046226163\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import AffixTagger\n",
    "from nltk.corpus import treebank\n",
    "testing = treebank.tagged_sents()[2000:]\n",
    "training= treebank.tagged_sents()[:7000]\n",
    "prefixtagger=AffixTagger(training,affix_length=4)\n",
    "prefixtagger3=AffixTagger(training,affix_length=3,backoff=prefixtagger)\n",
    "print(prefixtagger3.evaluate(testing))\n",
    "suffixtagger3=AffixTagger(training,affix_length=-3,backoff=prefixtagger3)\n",
    "print(suffixtagger3.evaluate(testing))\n",
    "suffixtagger4=AffixTagger(training,affix_length=-4,backoff=suffixtagger3)\n",
    "print(suffixtagger4.evaluate(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9882176652913768\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import tnt\n",
    "from nltk.corpus import treebank\n",
    "testing = treebank.tagged_sents()[2000:]\n",
    "training= treebank.tagged_sents()[:7000]\n",
    "tnt_tagger=tnt.TnT()\n",
    "tnt_tagger.train(training)\n",
    "print(tnt_tagger.evaluate(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9882176652913768\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import tnt\n",
    "from nltk.corpus import treebank\n",
    "testing = treebank.tagged_sents()[2000:]\n",
    "training= treebank.tagged_sents()[:7000]\n",
    "tnt_tagger=tnt.TnT()\n",
    "unknown=DefaultTagger('NN')\n",
    "tagger_tnt=tnt.TnT(unk=unknown,Trained=True)\n",
    "tnt_tagger.train(training)\n",
    "print(tnt_tagger.evaluate(testing))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT wise/JJ small/JJ girl/NN of/IN village/NN)\n",
      "  became/VBD\n",
      "  (NP leader/NN))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sent=[(\"A\",\"DT\"),(\"wise\", \"JJ\"), (\"small\", \"JJ\"),(\"girl\", \"NN\"), (\"of\", \"IN\"), (\"village\", \"N\"),  (\"became\", \"VBD\"), (\"leader\", \"NN\")]\n",
    "sent=[(\"A\",\"DT\"),(\"wise\", \"JJ\"), (\"small\", \"JJ\"),(\"girl\", \"NN\"), (\"of\", \"IN\"), (\"village\", \"NN\"),  (\"became\", \"VBD\"), (\"leader\", \"NN\")]\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN><IN>?<NN>*}\"\n",
    "find = nltk.RegexpParser(grammar)\n",
    "res = find.parse(sent)\n",
    "print(res)\n",
    "res.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP financial/NN year/NN account/NN summary/NN))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "noun1=[(\"financial\",\"NN\"),(\"year\",\"NN\"),(\"account\",\"NN\"),(\"summary\",\"NN\")]\n",
    "gram=\"NP:{<NN>+}\"\n",
    "find = nltk.RegexpParser(gram)\n",
    "print(find.parse(noun1))\n",
    "x=find.parse(noun1)\n",
    "x.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
